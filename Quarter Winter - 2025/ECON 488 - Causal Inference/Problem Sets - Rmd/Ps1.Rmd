---
title: "Econ 488, First problem set"
output: 
  pdf_document:
    keep_tex: true
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Solutions:
1. Subset of a population where the outcomes are indecent and identically distributed to one sample and another sample

2. Yes, we do expect the every Y_i to be unaffected to each other in terms of drawing outcome and their chance of being drawn. If {Y_1, Y_2} or {Y_1, Y10} are being drawn from a consecutive drawn without clearing draw box, then we would still say they are randomly sampled

3. Some possible DV and CV one can have from drawing 2 customers for December spending information collection
Discrete: average spending during December
Continuous: daily spending during December

4.

```{r}
set.seed(1)

# Given N(mu = 5, sigma = 1), a continuous rv, we want to find the mean (5). To prove that mean is 5, we can use expected value formal definition: E(X) = \int x*f_x(x) dx, where f_x(x) is the probability density function of X
N5_1 <- function(x) {
  x * dnorm(x, mean = 5, sd = 1)
}

EX <- integrate (N5_1, lower = 1, 
                 upper = Inf)
cat('The expected value of N(mu = 5, sigma = 1):', EX$value, '\n') # use str(EX) to check attributes

# Given Pr(Z = 1) + P(Z = 0) = 1, a discrete rv, we want to find the mean. To prove that mean is 0.5, we have E(Z) = y_z1p_1 + y_z2p2, where we expect p1 = p2 if they are randomly distributed 
ber_values <- c(0:1)
prob1_2 <- 1/length(ber_values)

BT <- ber_values[1]* prob1_2  + ber_values[2]*prob1_2
cat('The expected value of vernoulie trails of 0,1:', BT, '\n')
```
5. (not sure)
i.
Given $f)\cdot ) = g(\mu; \theta)$ is a probabiltiy density funciton (pdf) in a parametric form , to deuce the mathematical definition of $\theta$, a scaler, we...

will inversely produce the $\hat{\theta} =$ rest of the function taking $y_i$

ii.
Estimators are deduced based on random samples (Y_i), so the estimator itself would be random as well

iii.
estimator is the formal function definition of our distribution to approximate the true result, whereas estimate is the empirical result corresponding to the estimator

6.
Unbiased estimator is an estimator that give use the difference $E[\hat{\theta}]- \theta = 0$
- we access the unbiasness  haveing finite sample property when the sample size increase will not effect its property and that the estimator is equal to the true parameter

side note (bias and variance): low bias estimator does not correlate toe low variance; an estimator ($\hat{\theta_}1$) can have large variance but low bias, whereas a estimator ($\theta_2$) can also have low variance but biased; sometimes we may favor estimator with small biased and low variance estimator ($\hat{\theta}_2$) over a an accurate but highe variance estimator ($\theta_1$). One must ask the 'percision' before actually choosing an estimator

side note (approximation): $f(\hat{\theta})$ can be approximated using Central Value Theorem and Monte Carlo simulation
  - MCS: the empirical distribution of $\hat{\theta}$ over different simulated samples given siez T (realization: $\{R_t}_{t =1}^T$); very accurate if the simulated samples is very large
  -CLT: the normal distribution approximation claim that becomes more accurate as sample increases; the accuracy is very estimator and T dependent

ref: https://bookdown.org/compfinezbook/introcompfinr/Finite-Sample-Properties-of-Estimators.html#estimated-standard-errors

7.
1. $E[W^{a}_{n}] = E[Y_1]$ if Y is iid. 
2.  $E[W^{b}_{n}] = E[Y_n]$  if Y is iid
3. $E[W^{c}_{n}] = E[(Y_n + Y_1)/2]$  if Y is iid (Yn = Y1 cancels out the 2 denominator)
4. $E[W^{d}_{n}] = E[(Y_n +...+ Y_1)/n]$  if Y is iid (Yn = ... = Y1 cancels out the n denominator)

Thus, a,b,c, and d estimators are unbiased

8. Define an consistent estimator. Explain why this is considered a ‘large-sample’ (also called asymptotic) property of an estimator?

A consistent estimator should have the probability of the difference in estimaor and true value evaluate to zero base on a error margin condition, which the difference should decrease as sample increases asymptotically 

sidenote (consistency):
- def: an estimator ($\hat{\theta}$) is consistent for $\theta$ (converges in prob to $\theta$) if for any $\epsilon > 0$, where epsilon is the margin of error/tolerance/threhold od the difference
  - this means our estimator - estimate must have lower than margin as T increase to get a probability of 0

$$ 
lim_{T \to \infty} Pr(|\hat{\theta} - \theta|> \epsilon) = 0
$$
- Laws of Large Number theorem is used to determine if an estimator is consistent or not
  - bias($\hat{\theta}, \theta) = 0, T \to \infty$
  - SE($\hat{\theta}) = 0, T \to \infty$
  
ref: https://bookdown.org/compfinezbook/introcompfinr/Asymptotic-Properties-of-Estimators.html

9. Simple formal formulation of the Law of Large Numbers (henceforth LLN)
$\bar{X} \xrightarrow{\text{P}} \mu, n \to \infty$
$lim_{n \to \infty} Pr(|\bar{X} - \mu| < \epsilon) = 1, lim_{n \to \infty} Pr(|\bar{X} - \mu| > \epsilon) = 0$

10. Simple formal formulation of the Central limit theorem.

sidenote (CLT)
- def (simple form): the sample average of a collection of iid. random variable ($X_1, ..., X_T$) with $E[X_i] = \mu$ and $var(X_i) = \sigma^2$

11. Simple formal formulation of the Law of Iterated Expectations (henceforth LIE)



13. Here is the R code I have used to answer this question:

```{r message=FALSE, echo=TRUE, results='hide'}
set.seed(1)

z <- rchisq(4, 1)
mean(z)

```


I found the average value of $z$ to be `r mean(z)`.

This webpage that describes code chunk options: https://rstudio.github.io/cheatsheets/html/rmarkdown.html